{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 파이썬으로 RNN 구현하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy로 rnn 층을 구현해보자\n",
    "\n",
    "# 의사 코드(pseudocode)를 작성해보자\n",
    "# 실제 동작하는 코드는 아님에 주의\n",
    "\n",
    "hidden_state_t = 0  # 초기 은닉 상태를 0(벡터)로 초기화\n",
    "for input_t in input_length:\n",
    "    # 각 시점마다 입력을 받는다. # 입력 데이터의 길이는 총 시점의 수(timesteps)가 된다.\n",
    "    output_t = tanh(input_t, hidden_state_t)    # 각 시점의 입력과 은닉 상태를 받아서 연산\n",
    "    hidden_state_t = output_t   # 계산 결과는 현재 시점의 은닉 상태가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 다시 실제 동작의 되는 코드로 구현해보자\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10      # 시점의 수. nlp에서는 보통 문장의 길이가 된다.\n",
    "input_size = 4      # 입력의 차원. nlp에서는 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8     # 은닉 상태의 크기. 메모리 셀의 용량이 됨 \n",
    "\n",
    "inputs = np.random.random((timesteps, input_size))  # 입력에 해당되는 2D 텐서 = (문장의 길이, 벡터의 차원)\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size, ))  # 초기 은닉 상태는 hidden_size 크기의 0벡터로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state_t)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향을 각 크기에 맞게 정의\n",
    "\n",
    "Wx = np.random.random((hidden_size, input_size))    # 은닉 상태의 크기 * 입력의 차원\n",
    "Wh = np.random.random((hidden_size, hidden_size))   # 은닉 상태의 크기 * 은닉 상태의 크기\n",
    "b = np.random.random((hidden_size, ))               # 은닉 상태의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 편향 크기 확인\n",
    "print(Wx.shape)\n",
    "print(Wh.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.dot(a,b) : a와 b의 벡터 내적\n",
    "# a = [[1,0],\n",
    "#     [8,-2]]    \n",
    "# b = [-1,5]\n",
    "# print(np.dot(a,b)) # (2,2) * (2,) = (2,)\n",
    "\n",
    "# a = [[1,0],\n",
    "#     [8,-2]]    \n",
    "# b = [[-1,5],\n",
    "#     [-3,4]]\n",
    "# print(np.dot(a,b)) # (2,2) * (2,2) = (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.95942055 0.99493557 0.96279293 0.95929744 0.99073959 0.98498971\n",
      "  0.99149886 0.99430774]\n",
      " [0.9999539  0.99976379 0.99984711 0.99994282 0.99993461 0.99999126\n",
      "  0.99998097 0.9999793 ]\n",
      " [0.99996914 0.99996653 0.9999671  0.99996667 0.99998845 0.99999943\n",
      "  0.99999333 0.99999884]\n",
      " [0.99997952 0.99997711 0.99996202 0.99997345 0.99999256 0.99999939\n",
      "  0.99999606 0.99999927]\n",
      " [0.99985372 0.99973379 0.99976749 0.99995514 0.99990871 0.99999767\n",
      "  0.99998389 0.99999185]\n",
      " [0.99997477 0.99998018 0.99996864 0.99998019 0.99999311 0.99999963\n",
      "  0.99999737 0.99999949]\n",
      " [0.99996539 0.99995468 0.99994246 0.99996886 0.99998507 0.99999912\n",
      "  0.99999398 0.9999985 ]\n",
      " [0.99998109 0.9999531  0.99993019 0.99996057 0.99998674 0.99999791\n",
      "  0.9999924  0.99999764]\n",
      " [0.99994855 0.99986605 0.99976184 0.99993988 0.99995985 0.99999652\n",
      "  0.9999825  0.99999505]\n",
      " [0.99996547 0.9999317  0.99995086 0.99995937 0.99997854 0.99999855\n",
      "  0.99998972 0.99999651]]\n"
     ]
    }
   ],
   "source": [
    "# 모든 시점의 은닉 상태를 출력하다고 가정하고 rnn을 동작\n",
    "\n",
    "total_hidden_states = []\n",
    "\n",
    "# timesteps = 10      # 시점의 수. nlp에서는 보통 문장의 길이가 된다.\n",
    "# input_size = 4      # 입력의 차원. nlp에서는 보통 단어 벡터의 차원이 된다.\n",
    "# hidden_size = 8     # 은닉 상태의 크기. 메모리 셀의 용량이 됨 \n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs : # 각 시점에 따라서 입력값이 입력됨\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b) \n",
    "    # (Wx * Xt) + (Wh * Ht-1) + bt\n",
    "    total_hidden_states.append(list(output_t))  # 각 시점의 은닉 상태의 값을 계속 축적\n",
    "    print(np.shape(total_hidden_states))    # 각 시점 t별 메모리 셀의 출력 크기는 (timestep, output_dim)\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
    "\n",
    "print(total_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stack에 대한 정리\n",
    "\n",
    "# a = [1,2,3]\n",
    "# b = [2,3,4]\n",
    "\n",
    "# c.shape = (100,200)\n",
    "# d.shape = (100,200)\n",
    "# e.shape = (100,200)\n",
    "\n",
    "# 1) hstack : 가로로 합치기\n",
    "# htack(a,b) = [1,2,3,2,3,4]\n",
    "\n",
    "# 2) vstack : 세로로 합지기\n",
    "# vstack(a,b) = [[1,2,3]\n",
    "#               ,[2,3,4]]\n",
    "\n",
    "# 3) dstack : 차원 방향으로 합치기\n",
    "# dstack([c,d,e]).shape = (100,200,3)   # 2차원끼리 합쳤는데로 3차원 만들어서 합침\n",
    "\n",
    "# 4) stack : 원하는 차원으로 합치기. 디폴트 값은 0\n",
    "# stack([c,d,e]) = (3,100,200)\n",
    "# stack([c,d,e], axis=1) = (100, 3, 200)\n",
    "# stack([c,d,e], axis=2) = (100, 200, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 파이토치의 nn.RNN()\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn 에서 도구를 불러와 해보자\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5      # 입력의 크기\n",
    "hidden_size = 8     # 은닉 상태의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 벡터 텐서의 정의\n",
    "# (배치 크기,    시점의 수,    매 시점마다 들어가는 입력의 크기)\n",
    "# (batch_suze,  time_steps,   input_size)\n",
    "inputs = torch.Tensor(1,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.RNN()으로 셀을 만들자\n",
    "\n",
    "cell = nn.RNN(input_size, hidden_size, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서를 rnn에 입력하여 출력을 확인해보자\n",
    "outputs, _status = cell(inputs)\n",
    "# outputs : 모든 시점의 은닉 상태\n",
    "# _status : 마지막 시점의 은닉 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "# outputs : 모든 시점의 은닉 상태\n",
    "\n",
    "# 10 번의 시점동안 8차원의 은닉상태가 출력되었다는 뜻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(_status.shape)\n",
    "# _status : 마지막 시점의 은닉 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 깊은 순환 신경망(Deep Recurrent Neural Network)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.RNN()에 인자인 num_layers에 값을 넣어 은닉층이 여러개인 rnn을 만들자\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, _status = cell(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "# outputs : 모든 시점의 은닉 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(_status.shape)\n",
    "# _status : 마지막 시점의 은닉 상태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 양방향 순환 신경망(Bidirectional Recurrent Neural Network)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양방향 순환 신경망은 시점 t의 출력값을 예측 할 때 이전 뿐만 아니라\n",
    "# 이후 데이터까지 이용해 예측하는 데 아이디어가 있다.\n",
    "\n",
    "# 영어 빈칸 채우기 문제에 비유하여 생각하면 된다.\n",
    "# Exercise is very effective at [          ] belly fat.\n",
    "\n",
    "# 1) reducing\n",
    "# 2) increasing\n",
    "# 3) multiplying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.RNN()의 인자인 bidirectional 값을 True로 하여 진행해보자\n",
    "\n",
    "inputs = torch.Tensor(1,10,5)\n",
    "\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, _status = cell(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "# outputs : 모든 시점의 은닉 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(_status.shape)\n",
    "# _status : 정방향으로는 마지막 시점의 은닉 상태, 역방향으로는 첫번쨰 시점\n",
    "# 두 값이 concat되었기 때문에 2배가 된다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd75c4cec139db88207ac7c4293bdcbae3543546183bc362ebf9b62ac7e89fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('chch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

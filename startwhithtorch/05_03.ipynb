{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# https://wikidocs.net/60572"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "# 03. 소프트맥스 회귀의 비용 함수 구현하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26c23013f00>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 소프트맥수 함수의 비용 함수를 로우-레벨로 구현해보자\r\n",
    "\r\n",
    "z = torch.FloatTensor([1,2,3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 위에서 정의한 텐서를 입력으로하는 소프트맥스 함수를 만들자\r\n",
    "\r\n",
    "hypothesis = F.softmax(z, dim=0)\r\n",
    "print(hypothesis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 원소들의 합을 확인하자\r\n",
    "print(hypothesis.sum())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# ------------------\r\n",
    "# 비용 함수를 직접 구현하자\r\n",
    "# 3*5 크기의 텐서 만들기\r\n",
    "z = torch.rand(3, 5, requires_grad=True)\r\n",
    "print(z)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.4550, 0.5725, 0.4980, 0.9371, 0.6556],\n",
      "        [0.3138, 0.1980, 0.4162, 0.2843, 0.3398],\n",
      "        [0.5239, 0.7981, 0.7718, 0.0112, 0.8100]], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# 이 텐서를 입력으로 하는 소프트맥수 함수를 적용\r\n",
    "# 단 각 샘플에 대해 소프트맥스를 적용하여야 하므로 dim = 1 으로 적용\r\n",
    "hypothesis = F.softmax(z, dim=1)\r\n",
    "print(hypothesis)\r\n",
    "print([i.sum() for i in hypothesis])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.1664, 0.1871, 0.1737, 0.2695, 0.2033],\n",
      "        [0.2002, 0.1783, 0.2218, 0.1944, 0.2054],\n",
      "        [0.1809, 0.2380, 0.2318, 0.1084, 0.2409]], grad_fn=<SoftmaxBackward>)\n",
      "[tensor(1., grad_fn=<SumBackward0>), tensor(1., grad_fn=<SumBackward0>), tensor(1., grad_fn=<SumBackward0>)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# 각 샘플에 대하여 임의의 레이블을 만들자\r\n",
    "y = torch.randint(5, (3,)).long()   # long 타입으로 변환\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([3, 0, 4])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# 각 레이블에 대하여 원핫 인코딩을 수행\r\n",
    "y_one_hot = torch.zeros_like(hypothesis)    # 닮고 싶은 크기의 array 의 zero array를 만든다.\r\n",
    "print(y_one_hot.scatter_(1, y.unsqueeze(1), 1))\r\n",
    "# scatter_ 은 inplace=True인 상태로 scatter와 다르게 바로 해당 tensor에 적용된다.\r\n",
    "\r\n",
    "print(y.unsqueeze(1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[3],\n",
      "        [0],\n",
      "        [4]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# 비용 함수 코드로 구현\r\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\r\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.4478, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "## 하이레벨로 소프트맥스 비용 함수 구현하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# F.softmax() + torch.log() = F.log_softmax()\r\n",
    "# ---------------------------------------------------\r\n",
    "# low level 에서는 소프트맥스 함수의 결과에 로그를 씌울 때 다음과 같이 사용했다\r\n",
    "torch.log(F.softmax(z, dim=1))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.7935, -1.6760, -1.7504, -1.3114, -1.5929],\n",
       "        [-1.6086, -1.7244, -1.5062, -1.6381, -1.5826],\n",
       "        [-1.7096, -1.4354, -1.4617, -2.2223, -1.4236]], grad_fn=<LogBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# 그런데 토치에서는 두 개의 함수를 결합한 F.log_softmax 라는 도구를 제공한다\r\n",
    "# High level\r\n",
    "F.log_softmax(z, dim=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.7935, -1.6760, -1.7504, -1.3114, -1.5929],\n",
       "        [-1.6086, -1.7244, -1.5062, -1.6381, -1.5826],\n",
       "        [-1.7096, -1.4354, -1.4617, -2.2223, -1.4236]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 위 두 출력 결과가 동일한 것을 확인"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# F.log_softmax() + F.nll_loss() = F.cross_entropy()\r\n",
    "# ---------------------------------------------------\r\n",
    "# 이번에는 비용함수를 보자\r\n",
    "\r\n",
    "# low level\r\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.4478, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# 위 수식에서 torch.log(F.softmax(z, dim=1))를 방금 배운 F.log_softmax()로 대체할 수 있다.\r\n",
    "\r\n",
    "(y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.4478, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 두 출력 결과가 동일한 것을 확인"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# --------------------------------------------------\r\n",
    "# F.nll_loss() 사용하기\r\n",
    "\r\n",
    "# 더 간단하게 Negative log Likelihood를 사용할 수 있다. --> nll_loss\r\n",
    "# nll_loss는 원-핫 벡터를 넣을 필요 없이 바로 실제값을 인자로 사용한다\r\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.4478, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# --------------------------------------------------\r\n",
    "# F.cross_entropy() 사용하기\r\n",
    "\r\n",
    "# F.cross_entropy는 log_softmax와 nll_loss를 포함하고 있다\r\n",
    "F.cross_entropy(z, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.4478, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "fdd75c4cec139db88207ac7c4293bdcbae3543546183bc362ebf9b62ac7e89fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
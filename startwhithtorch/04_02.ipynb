{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# https://wikidocs.net/58686"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "# 02. nn.Module로 구현하는 로지스틱 회귀"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# nn.linear() 의 결과를 nn.sigmoid() 거치게 하여 로지스틱 회귀 가설을 만들자"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 라이브러리\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 랜덤시드 고정\r\n",
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2754ac53f60>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# x_train, y_train 데이터를 텐서로 선언\r\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\r\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\r\n",
    "x_train = torch.FloatTensor(x_data)\r\n",
    "y_train = torch.FloatTensor(y_data)\r\n",
    "\r\n",
    "print(x_train.shape)\r\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# nn.Sequential() 을 이용해서 nn.linear와 nn.sigmoid를 이어주자\r\n",
    "model = nn.Sequential(\r\n",
    "    nn.Linear(2, 1),    # input_dim = 2, output_dim = 1\r\n",
    "    nn.Sigmoid()        # 출력을 시그모이드 함수를 거친다\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 현재 w, b는 초기화 된 상태. 훈련 데이터를 넣어 예측값을 확인하자\r\n",
    "model(x_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4020],\n",
       "        [0.4147],\n",
       "        [0.6556],\n",
       "        [0.5948],\n",
       "        [0.6788],\n",
       "        [0.8061]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# y_train과 같은 6*1 형태의 텐서가 나온다.\r\n",
    "# 이제 경사 하강법을 사용하여 훈련해보자\r\n",
    "\r\n",
    "# optimizer 설정\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    # H(x) 계산\r\n",
    "    hypothesis = model(x_train)\r\n",
    "\r\n",
    "    # cost 계산\r\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\r\n",
    "\r\n",
    "    # cost로 H(x) 계산\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    # 20번 마다 로그 출력\r\n",
    "    if epoch % 20 == 0:\r\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])  # 0.5가 넘을 때만 True로\r\n",
    "        correct_prediction = prediction.float() == y_train   # 예측값이 실제값과 일치하는 경우만 True로 간주\r\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)        # 정확도를 계산\r\n",
    "        print('Epoch : {:4d}/{}   Cost : {:.6f}   Accuracy : {:2.2f}%'.format(\r\n",
    "            epoch, nb_epochs, cost.item(), accuracy*100\r\n",
    "        ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch :    0/1000   Cost : 0.539713   Accuracy : 83.33%\n",
      "Epoch :   20/1000   Cost : 0.441875   Accuracy : 66.67%\n",
      "Epoch :   40/1000   Cost : 0.316358   Accuracy : 83.33%\n",
      "Epoch :   60/1000   Cost : 0.220498   Accuracy : 100.00%\n",
      "Epoch :   80/1000   Cost : 0.157299   Accuracy : 100.00%\n",
      "Epoch :  100/1000   Cost : 0.134272   Accuracy : 100.00%\n",
      "Epoch :  120/1000   Cost : 0.118297   Accuracy : 100.00%\n",
      "Epoch :  140/1000   Cost : 0.105779   Accuracy : 100.00%\n",
      "Epoch :  160/1000   Cost : 0.095704   Accuracy : 100.00%\n",
      "Epoch :  180/1000   Cost : 0.087420   Accuracy : 100.00%\n",
      "Epoch :  200/1000   Cost : 0.080486   Accuracy : 100.00%\n",
      "Epoch :  220/1000   Cost : 0.074595   Accuracy : 100.00%\n",
      "Epoch :  240/1000   Cost : 0.069526   Accuracy : 100.00%\n",
      "Epoch :  260/1000   Cost : 0.065118   Accuracy : 100.00%\n",
      "Epoch :  280/1000   Cost : 0.061247   Accuracy : 100.00%\n",
      "Epoch :  300/1000   Cost : 0.057820   Accuracy : 100.00%\n",
      "Epoch :  320/1000   Cost : 0.054764   Accuracy : 100.00%\n",
      "Epoch :  340/1000   Cost : 0.052022   Accuracy : 100.00%\n",
      "Epoch :  360/1000   Cost : 0.049546   Accuracy : 100.00%\n",
      "Epoch :  380/1000   Cost : 0.047299   Accuracy : 100.00%\n",
      "Epoch :  400/1000   Cost : 0.045251   Accuracy : 100.00%\n",
      "Epoch :  420/1000   Cost : 0.043376   Accuracy : 100.00%\n",
      "Epoch :  440/1000   Cost : 0.041653   Accuracy : 100.00%\n",
      "Epoch :  460/1000   Cost : 0.040064   Accuracy : 100.00%\n",
      "Epoch :  480/1000   Cost : 0.038593   Accuracy : 100.00%\n",
      "Epoch :  500/1000   Cost : 0.037228   Accuracy : 100.00%\n",
      "Epoch :  520/1000   Cost : 0.035958   Accuracy : 100.00%\n",
      "Epoch :  540/1000   Cost : 0.034773   Accuracy : 100.00%\n",
      "Epoch :  560/1000   Cost : 0.033664   Accuracy : 100.00%\n",
      "Epoch :  580/1000   Cost : 0.032625   Accuracy : 100.00%\n",
      "Epoch :  600/1000   Cost : 0.031649   Accuracy : 100.00%\n",
      "Epoch :  620/1000   Cost : 0.030730   Accuracy : 100.00%\n",
      "Epoch :  640/1000   Cost : 0.029864   Accuracy : 100.00%\n",
      "Epoch :  660/1000   Cost : 0.029046   Accuracy : 100.00%\n",
      "Epoch :  680/1000   Cost : 0.028272   Accuracy : 100.00%\n",
      "Epoch :  700/1000   Cost : 0.027538   Accuracy : 100.00%\n",
      "Epoch :  720/1000   Cost : 0.026842   Accuracy : 100.00%\n",
      "Epoch :  740/1000   Cost : 0.026181   Accuracy : 100.00%\n",
      "Epoch :  760/1000   Cost : 0.025552   Accuracy : 100.00%\n",
      "Epoch :  780/1000   Cost : 0.024952   Accuracy : 100.00%\n",
      "Epoch :  800/1000   Cost : 0.024381   Accuracy : 100.00%\n",
      "Epoch :  820/1000   Cost : 0.023835   Accuracy : 100.00%\n",
      "Epoch :  840/1000   Cost : 0.023313   Accuracy : 100.00%\n",
      "Epoch :  860/1000   Cost : 0.022814   Accuracy : 100.00%\n",
      "Epoch :  880/1000   Cost : 0.022336   Accuracy : 100.00%\n",
      "Epoch :  900/1000   Cost : 0.021877   Accuracy : 100.00%\n",
      "Epoch :  920/1000   Cost : 0.021437   Accuracy : 100.00%\n",
      "Epoch :  940/1000   Cost : 0.021015   Accuracy : 100.00%\n",
      "Epoch :  960/1000   Cost : 0.020609   Accuracy : 100.00%\n",
      "Epoch :  980/1000   Cost : 0.020219   Accuracy : 100.00%\n",
      "Epoch : 1000/1000   Cost : 0.019843   Accuracy : 100.00%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 기존의 데이터를 가지고 예측해보자\r\n",
    "model(x_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[2.7616e-04],\n",
       "        [3.1595e-02],\n",
       "        [3.8959e-02],\n",
       "        [9.5624e-01],\n",
       "        [9.9823e-01],\n",
       "        [9.9969e-01]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 훈련 후의 w ,b 를 확인해보자\r\n",
    "print(list(model.parameters()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Parameter containing:\n",
      "tensor([[3.2534, 1.5181]], requires_grad=True), Parameter containing:\n",
      "tensor([-14.4839], requires_grad=True)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 결과적으로\r\n",
    "# 위의 인공 신경망은 다음과 같은 다중 로지스틱 회귀를 표현한다\r\n",
    "\r\n",
    "# H(x) = sigmoid(x1w1 + x2w2 + b)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "fdd75c4cec139db88207ac7c4293bdcbae3543546183bc362ebf9b62ac7e89fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
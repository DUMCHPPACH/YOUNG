{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wikidocs.net/60691"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. IMDB 리뷰 감성 분류하기(IMDB Movie Review Sentiment Analysis)\n",
    "\n",
    "참고 논문: http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 사이트 IMDB의 리뷰 데이터를 이용해 1 긍정 , 0 부정을 분류해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data, datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24de43f4370>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤시드 고정\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 변수\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cuda\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 토치텍스트를 이용한 전처리\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드하기 : torchtext.data\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "# sequential: 순차적 데이터\n",
    "# batch_first: 신경망에 입력되는 텐서의 첫번째 차원값이 batch_size가 되도록\n",
    "# lower: 입력되는 모든 알파벳이 소문자가 되도록\n",
    "LABEL = data.Field(sequential=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 로드 및 분할하기 : torchtext.datasets\n",
    "\n",
    "# 전체 데이터를 훈련, 테스트로 나누기(8:2)\n",
    "path = 'D:/chchdata/dataset/imdb_data'\n",
    "trainset, testset = datasets.IMDB.splits(root=path, text_field = TEXT, label_field= LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset의 구성 요소 출력:  {'text': <torchtext.legacy.data.field.Field object at 0x0000024E5D273160>, 'label': <torchtext.legacy.data.field.Field object at 0x0000024E5D273100>}\n",
      "testset의 구성 요소 출력:  {'text': <torchtext.legacy.data.field.Field object at 0x0000024E5D273160>, 'label': <torchtext.legacy.data.field.Field object at 0x0000024E5D273100>}\n"
     ]
    }
   ],
   "source": [
    "print('trainset의 구성 요소 출력: ', trainset.fields)\n",
    "print('testset의 구성 요소 출력: ', testset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"teachers\".', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"teachers\".', 'the', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'i', 'immediately', 'recalled', '.........', 'at', '..........', 'high.', 'a', 'classic', 'line:', 'inspector:', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'student:', 'welcome', 'to', 'bromwell', 'high.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched.', 'what', 'a', 'pity', 'that', 'it', \"isn't!\"], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 훈련 샘플과 해당 샘플에 대한 레이블을 함께 출력\n",
    "print(vars(trainset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 단어 집합 만들기\n",
    "# 중복을 제거한 총 단어 집합 만들기\n",
    "\n",
    "TEXT.build_vocab(trainset, min_freq = 5) # 최소 5번 이상 등장한 것만 추가\n",
    "LABEL.build_vocab(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 46159\n",
      "클래스의 개수 : 2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('클래스의 개수 : {}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어와 정수가 저장되어 있는 dict 출력해보자\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 데이터 로더 만들기\n",
    "\n",
    "# 훈련, 테스트를 나누었으니 이번에는 검증까지 분리하자\n",
    "trainset, valset = trainset.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 배치 처리 + 단어 인덱스 번호 대체 를 제공하는 BucketIterator를 사용하자\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((trainset, valset, testset)\n",
    "                                                            , batch_size = BATCH_SIZE   # 64\n",
    "                                                            , shuffle=True\n",
    "                                                            , repeat=False) # 모든 에폭에 반복할지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 313\n",
      "테스트 데이터의 미니 배치의 개수 : 391\n",
      "검증 데이터의 미니 배치의 개수 : 79\n"
     ]
    }
   ],
   "source": [
    "# 64 batch size로 묶인 데이터의 형태 확인\n",
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 807])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 미니 배치 크기 확인\n",
    "batch = next(iter(train_iter))\n",
    "print(batch.text.shape)\n",
    "\n",
    "# 프린트 할 때 마다 train_iter에서 하나씩 빠지므로 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RNN 모델 구현하기\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        # 첫번째 hidden state를 0벡터로 초기화\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
    "        x, _ = self.gru(x, h_0)\n",
    "        # (배치큭, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time_step의 은닉 상태만 가져옴\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        # (배치크기, 은닉 상태의 크기) -> 배치 크기, 출력층의 크기) 로 변환\n",
    "        logit = self.out(h_t)\n",
    "        return logit\n",
    "\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(n_layers = 1\n",
    "            , hidden_dim = 256\n",
    "            , n_vocab = vocab_size\n",
    "            , embed_dim = 28\n",
    "            , n_classes = n_classes\n",
    "            , dropout_p = 0.5).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 이용한 훈련 함수\n",
    "\n",
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)  # 1과 2였던 label 값에서 모두 1씩 빼고 대체하여 레이블 값을 0과 1로 변환\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] val loss :  0.69 | val accuracy : 49.96\n",
      "[Epoch: 2] val loss :  0.69 | val accuracy : 50.22\n",
      "[Epoch: 3] val loss :  0.69 | val accuracy : 49.18\n",
      "[Epoch: 4] val loss :  0.69 | val accuracy : 49.32\n",
      "[Epoch: 5] val loss :  0.69 | val accuracy : 51.16\n",
      "[Epoch: 6] val loss :  0.69 | val accuracy : 50.36\n",
      "[Epoch: 7] val loss :  0.70 | val accuracy : 50.28\n",
      "[Epoch: 8] val loss :  0.72 | val accuracy : 50.02\n",
      "[Epoch: 9] val loss :  0.73 | val accuracy : 51.32\n",
      "[Epoch: 10] val loss :  0.74 | val accuracy : 49.74\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
    "\n",
    "    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\n",
    "\n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"D:/chchdata/h5/snapshot\"):\n",
    "            os.makedirs(\"D:/chchdata/h5/snapshot\")\n",
    "        torch.save(model.state_dict(), 'D:/chchdata/h5/snapshot/txtclassification.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 오차:  0.69 | 테스트 정확도: 47.78\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('D:/chchdata/h5/snapshot/txtclassification.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iter)\n",
    "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서의 크기 : torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# 4. 마지막 time step의 hidden state 가져오는 것 이해하기\n",
    "\n",
    "# GRU의 마지막 time_step의 hidden_state를 가져오는 코드를 임의의 텐서를 사용해 실습해보자\n",
    "import torch\n",
    "inputs = torch.rand(3, 4, 5) # 임의의 3차원 텐서 생성\n",
    "\n",
    "print('텐서의 크기 :',inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0235, 0.4590, 0.9496, 0.8781, 0.8022],\n",
      "         [0.4896, 0.3085, 0.3007, 0.0315, 0.9417],\n",
      "         [0.3339, 0.0917, 0.2220, 0.6460, 0.9839],\n",
      "         [0.3196, 0.7640, 0.0758, 0.0167, 0.1228]],\n",
      "\n",
      "        [[0.6639, 0.6345, 0.9903, 0.1532, 0.9122],\n",
      "         [0.9826, 0.3369, 0.0994, 0.0434, 0.2157],\n",
      "         [0.8979, 0.3712, 0.8974, 0.6721, 0.4596],\n",
      "         [0.1289, 0.7171, 0.7642, 0.5292, 0.2155]],\n",
      "\n",
      "        [[0.3976, 0.9681, 0.6180, 0.6289, 0.9304],\n",
      "         [0.6978, 0.1229, 0.1887, 0.3612, 0.2288],\n",
      "         [0.2392, 0.1942, 0.1158, 0.8707, 0.6355],\n",
      "         [0.9193, 0.0402, 0.2062, 0.8157, 0.7216]]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서값을 출력해보자\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3196, 0.7640, 0.0758, 0.0167, 0.1228],\n",
      "        [0.1289, 0.7171, 0.7642, 0.5292, 0.2155],\n",
      "        [0.9193, 0.0402, 0.2062, 0.8157, 0.7216]])\n"
     ]
    }
   ],
   "source": [
    "# 이제 [:, -1, :] 연산을 해보자\n",
    "print(inputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0차원의 각 마지막 줄만 가져왔다\n",
    "\n",
    "# 바뀐 형태도 보자\n",
    "print('텐서의 크기 :',inputs[:, -1, :].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd75c4cec139db88207ac7c4293bdcbae3543546183bc362ebf9b62ac7e89fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('chch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 09. 과적합(Overfitting)을 막는 방법들\r\n",
    "---\r\n",
    "\r\n",
    "훈련 데이터에 대한 정확도는 높지만 새로운 데이터(검증 혹은 테스트)에 대해서는 제대로 동작하지 않는 상태. 이는 모델이 학습 데이터를 불필요할 정도로 과하게 암기하여 훈련 데이터에 포한된 노이즈까지 학습한 상태라고 해석 할 수 있다.\r\n",
    "\r\n",
    "### 1. 데이터의 양을 늘리기\r\n",
    "데이터의 양이 적을 수록 데이터의 특정 패턴이나 노이즈까지 쉽게 암기 할 수 있다.  \r\n",
    "데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습한다.  \r\n",
    "\r\n",
    "### 2. 모델의 복잡도 줄이기\r\n",
    "인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정된다.  \r\n",
    "과적합이 발생했을 때 모델의 복잡도를 줄이면 현상을 어느정도 방지할 수 있다.  \r\n",
    "\r\n",
    "### 3. 가중치 규제(regularization) 적용하기\r\n",
    "복작한 모델이 간단한 모델보다 과적합될 가능성이 높다. \r\n",
    "- L1 규제: 가중치 w들의 절대값 합계를 비용 함수에 추가한다.\r\n",
    "- L2 규제: 모든 가중치 w들의 제곱합을 비용 함수에 추가한다.\r\n",
    "\r\n",
    "### 4. 드롭아웃\r\n",
    "드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다.  \r\n",
    "예를 들어 드롭아웃 비율을 0.2로 한다면 학습 과정마다 랜덤으로 20%의 뉴런을 사용하지 않고 80%만 사용한다.  \r\n",
    "주의할 점은, 드롭아웃은 신경망 학습 시에만 사용하고 예측 시에는 사용하지 않는 것이 일반적이라는 것이다.  \r\n",
    "드롭아웃을 적용하면 학습시에 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지하고 매번 랜덤 선택으로 뉴런을 사용하지 않으므로 서로 다른 신경망들을 망상블하여 사용하는 것 같은 효과를 낼 수 있다.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
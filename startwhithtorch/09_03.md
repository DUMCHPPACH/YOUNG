# Word2Vec
---

원-핫 인코더는 단어 간 유사도를 계산 할 수 없다는 단점이 존재.  
하여 단어간 유사도를 반영할 수 있도록 단어의 의미를 벡터화 하는 방법이 필요.  
가장 대표적인 방법이 word2vec.  
아래 사이트에서 연산을 해보면,  
[한국어 단어에 대한 벡터 연산을 할 수 있는 사이트](http://w.elnn.kr/search/)  
고양이 + 애교 = 강아지  
한국 - 서울 + 도쿄 = 일본  
이라는 결과를 반환해줌.  
이런 결과는 각 단어 벡터가 단어간 유사도를 반영한 값을 가지고 있기 때문.  

---
### 1. 희소 표현(Sparse Representation)  
원=핫 벡터는 표현하고자 하는 단어의 인덱스만 1이고 나머지 인덱스는 전부 0으로 표현되는  
희소 표현(sparse representation) 방법을 사용하는 희소 벡터(sparse representation)  
이를 위한 대안으로 단어의 '의미'를 다차원 공간에 벡터화하는 분산 표현(distributed representaion) 방법을 고안.  
그리고 이런 분산 표현을 이용하여 단어의 유사도를 벡터화하는 작업은 워드 임베딩에 속함.  
하여 이런 방법을 통해 표현된 벡터를 임베딩 벡터(embedding vector)로 칭함,  

---
### 2. 분산 표현(Distributed Representation)  
분산 표현 방법은 기본적으로 분포 가설(distribution hypothesis)이라는 가정 하에 만들어진 표현 방법.  
'비슷한 위치에서 등장하는 단어들은 비슷한 의미를 갖는다.'라는 게 그것.  
분산 표현은 분포 가설을 이용해 단어셋을 학습하고 벡터에 단어의 의미를 여러 차원에 분산하여 표현한다.  
분산 표현은 사용자가 설정한 차원을 가지는 벡터가 되면서 각 차원은 실수형의 값을 갖게 됨.  

분산 표현은 저차원에 **단어의 의미를 여러 차원에다가 분산**하여 표현하는 것임.  
이런 표현 방법을 사용하면 **단어 간 유사도**를 계산할 수 있음.  

이 전에는 NNLM, RNNLM 등이 있었으나 요즘에는 해당 방법들의 속도를 개선시킨 Word2Vec이 많이 쓰이고 있음.  

---
### 3. CBOW(Continuous Bag of Words)
Word2Vec에는 CBOW(continuous bag of words)와 skip-gram 두 가지 방식이 있음.
- CBOW : 주변에 있는 단어들을 가지고 중간에 있는 단어들을 예측하는 방법  
- Skip-Gram : 중간에 있는 단어로 주변 단어들을 예측하는 방법  


CBOW 먼저 이해하고 넘어가자.  
**예문: 'The fat cat sat on the mat'**  
CBOW는 {"The", "fat", "cat", "on", "the", "mat"}으로부터 sat을 예측하는 것.  
이때 예측해야 하는 sat을 중심 단어(centor word)라 하고,  
예측에 사용되는 단어들을 주변 단어(context word)라 한다.  
중심 단어를 예측하기 위하여, 앞 뒤로 몇 개의 단어를 볼지를 window라고 한다.  
  
  
예를 들어) window size = 2 , centor_word = sat 이라면  
앞의 두 단어인 fat, cat 그리고 뒤의 두 단어인 on, the를 참고한다.  
  
  
윈도우 크기가 n이라고 한다면 실제 중심 단어를 예측하기 위해 참고하는 주변 단어의 개수는 2n이 될 것이다.  
윈도우 크기를 정했다면, 윈도우를 계속 움직여서 주변 단어와 중심 단어를 바꿔가며 학습을 위한 데이터셋을 만드는데 이 방법을 sliding window 라고 한다.  
![image](https://user-images.githubusercontent.com/76423415/134446377-19960b61-152d-44c0-b8b7-8409dacae51e.png)


    

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 도구 임포트\r\n",
    "import torch\r\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 임의의 텐서 만들기 (1*1*28*28)\r\n",
    "\r\n",
    "inputs = torch.Tensor(1,1,28,28)\r\n",
    "print('inputs 텐서의 크기: {}'.format(inputs.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs 텐서의 크기: torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 첫번째 합성곱층 선언\r\n",
    "conv1 = nn.Conv2d(in_channels = 1,      # 입력받을 채널\r\n",
    "                  out_channels = 32,    # 내보낼 채널\r\n",
    "                  kernel_size = 3,      # 커널 사이즈(3,3)\r\n",
    "                  padding=1)            # 패딩사이즈(zero padding)\r\n",
    "\r\n",
    "print(conv1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 두번째 합성곱층 구현\r\n",
    "conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\r\n",
    "print(conv2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 풀링 선언\r\n",
    "pool = nn.MaxPool2d(2)      # 인자를 하나만 넣으면 kernel, stride로 모두 지정된다.\r\n",
    "print(pool)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 구현한 것들을 연결하여 모델 만들기\r\n",
    "\r\n",
    "# 첫번째 합성곱을을 통과한 inputs의 변화 보기\r\n",
    "out = conv1(inputs)\r\n",
    "print(out.shape)\r\n",
    "# inputs 텐서의 크기: torch.Size([1, 1, 28, 28])\r\n",
    "# >> torch.Size([1, 32, 28, 28])\r\n",
    "# 32채널 : conv1의 아웃채널을 32로 지정했기때문\r\n",
    "# 28,28 : padding 을 1로 설정하고 kernel size가 3*3이기 때문에 크기가 보존 됨"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 32, 28, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# maxpool을 통과한 값도 확인\r\n",
    "# MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\r\n",
    "out = pool(out)\r\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 32, 14, 14])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\lemon\\anaconda3\\envs\\chch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# conv2 를 통과한 사이즈도 확인\r\n",
    "out = conv2(out)\r\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 64, 14, 14])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# 다시 pooling 통과한 크기 확인\r\n",
    "out = pool(out)\r\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 64, 7, 7])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 결과로 나온 feature map을 1차원으로 펴주자\r\n",
    "# n번째 차원에 접근하게 해주는 .size(n)를 사용하자\r\n",
    "\r\n",
    "# out의 각 차원이 몇인지 확인\r\n",
    "print(out.size(0))\r\n",
    "print(out.size(1))\r\n",
    "print(out.size(2))\r\n",
    "print(out.size(3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "64\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# 이 값을 갖고 .view()를 사용해 텐서를 펼쳐보자  \r\n",
    "\r\n",
    "# 첫번째 차원인 배치 차원은 그대로 두고 나머지는 펼쳐라\r\n",
    "out = out.view(out.size(0), -1)\r\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3136])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# 펼친 feature map을 fully connected layer에 통과시키자\r\n",
    "\r\n",
    "fc = nn.Linear(3136, 10)\r\n",
    "out = fc(out)\r\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "fdd75c4cec139db88207ac7c4293bdcbae3543546183bc362ebf9b62ac7e89fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
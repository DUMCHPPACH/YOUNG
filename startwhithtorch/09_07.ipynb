{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 07. 파이토치(PyTorch)의 nn.Embedding()\r\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 파이토이체어 임베딩 벡터를 사용하는 큰 방법 두 가지\r\n",
    "# 1) 임베딩 층(layer)를 만들어서 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법\r\n",
    "# 2) 미리 사전에 훈련된 임베딩 벡터들을 가져와 사용하는 방법\r\n",
    "\r\n",
    "# 여기서는 전자에 대해 진행한다. nn.Embedding()을 이용"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 임베딩 층은 룩업 테이블이다.\r\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "임베딩 층의 입력으로 사용하기 위해서는 입력 시퀀스의 모든 단어는 정수 인코딩 된 상태여아 한다.  \r\n",
    "어떤 단어 -> 단어에 부여된 고유한 정수값 -> 임베딩 층 통과 -> 밀집 벡터(dense vector)  \r\n",
    "\r\n",
    "- 임베딩 층에서는 어떤 일이?  \r\n",
    "임베딩 층은 입력 정수에 대해 밀집 벡터로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련이 된다. 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트가 된다. 그리고 이 밀집 벡터를 임베딩 벡터라고 부른다.  \r\n",
    "\r\n",
    "- 정수는 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 건 어떤 뜻일까?  \r\n",
    "특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로 부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가진다.  \r\n",
    "\r\n",
    "![image](https://wikidocs.net/images/page/33793/lookup_table.PNG)  \r\n",
    "\r\n",
    "위 그림은 단어 great이 정수 인코딩 된 후 테이블로부터 해달 인덱스에 위치한 임베딩 벡터를 꺼내오는 모습을 보여준다.  \r\n",
    "임베딩 벡터의 차원: 4  \r\n",
    "great의 정수 인코딩: 1918  \r\n",
    "great이 사용한 임베딩 벡터의 행 번호: 1918  \r\n",
    "\r\n",
    "이 임베딩 벡터는 모델의 입력이 되고, 역전파 과정에서 단어 great의 임베딩 벡터값이 학습된다.  \r\n",
    "\r\n",
    "- 원-핫 벡터가 아니어도 괜찮은걸까?  \r\n",
    "파이토치는 단어를 정수 인덱스로 바꾸고 원-핫 벡터로 한 번 더 바꾸고 나서 임베딩 층의 입력에 사용하는 것이 아니라!  \r\n",
    "단어를 정수 인덱스로만 바꾼채로 임베딩 층의 입력으로 사용하더라도 룩업 테이블이 된 결과 즉 임베딩 벡터를 리턴할 수 있다.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# nn.Embedding()을 사용하지 않고 구현하며 이해해보자\r\n",
    "# 우선 임의의 문장으로부터 단어 집합을 만들고 각 단어에 정수를 부여하자\r\n",
    "\r\n",
    "train_data = 'you need to know how to code'\r\n",
    "word_set = set(train_data.split())  # 중복을 제거한 단어들의 집합인 단어 집합 생성\r\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수를 맵핑\r\n",
    "\r\n",
    "vocab['<unk>'] = 0  # 사전에 없는 단어는 0으로\r\n",
    "vocab['<pad>'] = 1  # padding은 1로\r\n",
    "print(vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'you': 2, 'code': 3, 'need': 4, 'to': 5, 'how': 6, 'know': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 이제 단어 집합의 크기를 행으로 갖는 임베딩 테이블을 구현하자\r\n",
    "# 여기서 임베딩 벡터의 차원은 3으로 정했다\r\n",
    "\r\n",
    "embedding_table = torch.FloatTensor([\r\n",
    "                               [ 0.0,  0.0,  0.0],      # unk\r\n",
    "                               [ 0.0,  0.0,  0.0],      # pad\r\n",
    "                               [ 0.2,  0.9,  0.3],\r\n",
    "                               [ 0.1,  0.5,  0.7],\r\n",
    "                               [ 0.2,  0.1,  0.8],\r\n",
    "                               [ 0.4,  0.1,  0.1],\r\n",
    "                               [ 0.1,  0.8,  0.9],\r\n",
    "                               [ 0.6,  0.1,  0.1]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 임의의 문장 you need to run 에 대해서 룩업 테이블을 통해 임베딩 벡터들을 가져와보자\r\n",
    "\r\n",
    "sample = 'you need to run'.split()\r\n",
    "idxes = []\r\n",
    "# 각 단어를 정수로 변환\r\n",
    "for word in sample:\r\n",
    "    try:\r\n",
    "        idxes.append(vocab[word])\r\n",
    "    except KeyError:    # 단어 집합에 없는 단어입 경우 <unk>로 대체 해라\r\n",
    "        idxes.append(vocab['<unk>'])\r\n",
    "idxes = torch.LongTensor(idxes)\r\n",
    "\r\n",
    "print('idxes: ', idxes)\r\n",
    "\r\n",
    "# 룩업 테이블\r\n",
    "lookup_result = embedding_table[idxes, :]   # 각 정수를 인덱스로 임베딩 테이블에서 값을 가져오자\r\n",
    "print(lookup_result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "idxes:  tensor([2, 4, 5, 0])\n",
      "tensor([[0.2000, 0.9000, 0.3000],\n",
      "        [0.2000, 0.1000, 0.8000],\n",
      "        [0.4000, 0.1000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 임베딩 층 사용하기\r\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 임베딩 층을 사용하는 경우를 보자 전처리는 동일하자\r\n",
    "\r\n",
    "train_data = 'you need to know how to code'\r\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\r\n",
    "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\r\n",
    "vocab['<unk>'] = 0\r\n",
    "vocab['<pad>'] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# nn.Embedding()을 통해 임베딩 테이블을 만들자\r\n",
    "import torch.nn as nn\r\n",
    "embedding_layer = nn.Embedding(num_embeddings= len(vocab)   # 임베딩을 할 단어들의 개수 =  집합의 크기\r\n",
    "                                , embedding_dim = 3         # 임베딩 할 벡터의 차원\r\n",
    "                                , padding_idx = 1           # 패딩의 인덱스\r\n",
    "                                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(embedding_layer.weight)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.7040, -1.0814,  0.8070],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3270, -0.9744, -0.1897],\n",
      "        [-1.2277,  0.1845, -0.3974],\n",
      "        [-0.1648,  0.7133, -0.7490],\n",
      "        [-0.6543,  0.0881, -0.7912],\n",
      "        [-0.7704, -0.4184,  0.6578],\n",
      "        [ 1.2818, -0.5012,  1.0980]], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('chch': conda)"
  },
  "interpreter": {
   "hash": "fdd75c4cec139db88207ac7c4293bdcbae3543546183bc362ebf9b62ac7e89fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
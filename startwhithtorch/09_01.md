# 01. NLP에서의 원-핫 인코딩(One-hot encoding)
---

컴퓨터는 문자보다는 숫자를 더 잘 처리 할 수 있다.  
이를 위해 자연어 처리에서 문자를 숫자로 바꾸는 여러 기법이 있다.  
원-핫 인코딩은 많은 기법 중 가장 기본적인 표현 방법이다.  
원-핫 인코딩에 앞서 단어 집합(vocabulary)에 대해 정의하자.  
단어 집합(vocaburaly)는 서로 다른 단어들의 집합이다.  
그러나 단어라는 정의에 좀 더 주목해야하는데,  
단어 집합에서는 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주한다.  

- 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 이를 단어 집합이라고 한다.  
- 이 단어 집합에 고유한 숫자를 부여하는 정수 인코딩을 진행한다.  
(텍스트에 단어가 500개라면 단어집합의 크기는 500이고 이 단어 집합의 단어들마다 1번부터 500번까지 인덱스를 부여한다.  
예를 들어, book은 15번 dog는 7번 love는 92번 books는 212번 등으로 부여한다.)  
- 아제 각 단어에 고유한 정수 인덱스를 부여했다면 이 숫자로 바뀐 단어들을 벡터로 어떻게 바꿔야 할까?


---
### 1. 원-핫 인코딩(One-hot encoding)이란?

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다.  
이렇게 표현된 벡터를 원-핫 벡터라고 한다.  

원-핫 인코딩을 두가지 과정으로 정리하면  
(1) 각 단어에 고유한 인덱스를 부여한다.  
(2) 표현하고 싶은 단어의 인덱스 위치에 1, 다른 단어의 인덱스에는 0을 부여한다.  

예를 들어 진행해보자  
문장 : 나는 자연어 처리를 배운다  
형태소 분석을 통한 토큰화: ['나', '는', '자연어', '처리', '를', '배운다']   
정수 인코딩하여 인덱스 부여: {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}  

원핫 진행읠 위한 함수 정의  
def one_hot_encoding(word, word2index):
       one_hot_vector = [0]*(len(word2index))
       index = word2index[word]
       one_hot_vector[index] = 1
       return one_hot_vector

결과물  
>> 나 -> [1,0,0,0,0,0]  
>> 는 -> [0,1,0,0,0,0]  
>> 자연어 -> [0,0,1,0,0,0]  
>> 등...


---
### 2. 원-핫 인코딩(One-hot encoding)의 한계

1. 비효율적인 저장 공간  
이러한 표현 방식은 단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점.  
원-핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수.  
가령, 단어가 1,000개인 코퍼스를 가지고 원-핫 벡터를 만들면 오든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 된다.  
가시 말해 모든 단어 각각은 하나의 값만 1을 갖고, 나머지 999개의 값은 0을 가져 저장 공간 측면에서 매우 비효율적  

2. 단어간 유사도 표현 불가  
늑대, 호랑이, 강아지, 고양이 라는 4개의 단어에 대해 원-핫 인코딩을 해서 각각 [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1] 으로 되었을 때.  
강아지-늑대가 유사하고 호랑이-고양이가 유사하다는 것을 표현할 수가 없다.  
극단적으로 강아지, 개, 냉장고에서 냉장고와 강아지라는 단어 중 개와 어떤 것이 비슷한지도 알 수 없다.  
**이는 검색 시스템 등에서 심각한 문제**  
호캉스라고 검색했을 때 이 검색어에서 호텔, 집캉스 등의 유사 단어에 대한 결과도 함께 보여줄 수 있어야 하는데,  
유사성을 계산할 수 없기 떄문이다.  

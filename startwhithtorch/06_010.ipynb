{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 10. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)\r\n",
    "---\r\n",
    "깊은 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상이 발생. 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 된다. 이를 기울기 소실이라 함.  \r\n",
    "반대의 경우로 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 발산하기도 한다. 이를 기울기 폭주라 함.\r\n",
    "\r\n",
    "\r\n",
    "### 1. ReLU와 ReLU의 변형들\r\n",
    "시그로이드 함수를 사용하면 입력의 절대닶이 클 경우에 시그모이드 함수의 출력값이 0또는 1에 수렴하면서 기울기가 0에 가까워진다. 그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생할 수 있다.  \r\n",
    "\r\n",
    "기울기 소실을 완화하는 간단한 방법은 윽닉층 활성화 함수로 시그모이드나 하이퍼볼릭탄젠츠 함수 대신에 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하는 것이다. \r\n",
    "\r\n",
    "\r\n",
    "### 2. 가중치 초기화(Weight initialization)\r\n",
    "같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라 모델의 훈련 결과가 달라지기도 한다. 가중치 초기화만 적절히 해줘도 기울기 소실과 같은 문제를 방지할 수 있다.  \r\n",
    "\r\n",
    "1. 세이비어 초기화(Xavier Initialization)\r\n",
    "   \r\n",
    "2. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
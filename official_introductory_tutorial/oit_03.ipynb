{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "oit_03.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.5 64-bit ('chch': conda)"
    },
    "interpreter": {
      "hash": "fdd75c4cec139db88207ac7c4293bdcbae3543546183bc362ebf9b62ac7e89fe"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungriKIM/chch/blob/main/official_introductory_tutorial/oit_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3강. 데이터 불러오기 + 커스터마이징"
      ],
      "metadata": {
        "id": "EozNwTailabF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# 라이브러리 불러오기\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "# 데이터를 불러오면서 바로 전처리를 할 수 있는 라이브러리 transforms\r\n",
        "import torchvision.transforms as tr \r\n",
        "# DataLoader: 데이터를 배치사이즈 형태로 만들어 학습에 이용할 수 있게함\r\n",
        "from torch.utils.data import DataLoader, Dataset\r\n",
        "import numpy as np"
      ],
      "outputs": [],
      "metadata": {
        "id": "KZvbPVbDlabH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파이토치에서 제공하는 데이터 불러오기\n",
        "---"
      ],
      "metadata": {
        "id": "JNiZeD9dlabH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# transform 을 미리 정의하자\r\n",
        "# 전처리 할 때 compose 안에 있는 순서대로 진행\r\n",
        "# Resize : 8,8로 리사이즈 , ToTensor: 텐서로 변환\r\n",
        "transf = tr.Compose([tr.Resize(8), tr.ToTensor()])"
      ],
      "outputs": [],
      "metadata": {
        "id": "ExmbfIhBlabI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='D:/chchdata/dataset', train=True, download=True, transform=transf)\r\n",
        "testset = torchvision.datasets.CIFAR10(root='D:/chchdata/dataset', train=False, download=True, transform=transf)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "metadata": {
        "id": "BQImdkA2labI",
        "outputId": "45e2eb23-60f4-4977-c791-ae3075ae85de"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# 트레인 셋의 첫번째를 불러와서 사이즈 확인\r\n",
        "trainset[0][0].size()\r\n",
        "# 3: 컬러라 3채널\r\n",
        "# 8,8 리사이즈 한 것"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "id": "n4MSMP2JlabK",
        "outputId": "05f8f1da-a44e-4382-a563-e23fc5264695"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# 배치 사이즈 별로 데이터를 불러오자\r\n",
        "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=1)\r\n",
        "TestLoader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dTcgtD5_labL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# dataloader의 길이를 확인해서 배치 사이즈로 잘 잘라졌는지 확인\r\n",
        "len(trainloader)\r\n",
        "\r\n",
        "# 전체 데이터의 수가 50,000개이기 때문에 배치 사이즈를 50으로 했을 때 길이가 1,000이 나오게 된다."
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "id": "wNAN2rHDlabM",
        "outputId": "ea4b54e1-d0d7-4ccf-ccf2-3a31b2cb8a9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# train 안의 실제 값을 확인하고 싶다면\r\n",
        "dataiter = iter(trainloader)\r\n",
        "images, labels = dataiter.next() # next : 한 묶음을 가져오겠다.\r\n",
        "\r\n",
        "# 이렇게 불러온 이미지 사이즈를 확인하면\r\n",
        "print(images.size())\r\n",
        "# torch.Size([50, 3, 8, 8]) : 50개의 배치 사이즈로 한 묶음인 3채널의 8,8 크기의 이미지"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 3, 8, 8])\n"
          ]
        }
      ],
      "metadata": {
        "id": "br6xuIiHlabO",
        "outputId": "2cc35877-7a4b-46b2-9bfb-ebbe89752d09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 개인 데이터 사용하기  \n",
        "---"
      ],
      "metadata": {
        "id": "oND2-n7HlabP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# ImageFolder를 이용한 데이터 불러오기 \r\n",
        "genderdir = 'D:/aidata/gender' # 안에 female 과 male 이 나뉘어져 정리되어 있는 상태\r\n",
        "transf = tr.Compose([tr.Resize(16), tr.ToTensor()]) # 전처리를 미리 정의\r\n",
        "trainset = torchvision.datasets.ImageFolder(root = genderdir, transform=transf) # 이미지폴더를 이용해 루트안의 데이터를 transf로 지정한 전처리 해서 불러오기\r\n",
        "trainloader = DataLoader(trainset, batch_size=1, shuffle=False, num_workers=1)\r\n",
        "print(len(trainloader))\r\n",
        "# 한 폴더 당 10개씩이고 batch_size가 1이니 총 20"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "metadata": {
        "id": "DJWnoxxRlabP",
        "outputId": "a17bd678-1441-4688-b430-ac62f5bc9aae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# 사이즈도 확인해보자\r\n",
        "trainset[0][0].size()\r\n",
        "# resize한 16으로 나옴"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "id": "ue2eVIjZlabQ",
        "outputId": "09c6b423-a70f-4ecb-a557-89ed6a57ad3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 개인 데이터 사용 방법\n",
        "---"
      ],
      "metadata": {
        "id": "1WjWQlEVlabQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# numpy로 데이터가 들어왔다고 가정\r\n",
        "train_images = np.random.randint(256, size=(20, 32, 32, 3))\r\n",
        "train_labels = np.random.randint(2, size=(20, 1))\r\n",
        "\r\n",
        "print(train_images.shape, train_labels.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 32, 32, 3) (20, 1)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHJaVfPSlabQ",
        "outputId": "207bcfc0-adb9-4353-cde8-5af3508ac2b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# 외부의 데이터를 상속 받을 수 있도록 하는 클래스 정의\r\n",
        "# 원하는 전처리 형태로 바꿔서 사용 가능\r\n",
        "\r\n",
        "class TensorData(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, x_data, y_data):\r\n",
        "        self.x_data = torch.FloatTensor(x_data) # FloatTensor : 2비트의 부동 소수점\r\n",
        "        self.x_data = self.x_data.permute(0,3,1,2) # permute를 이용해 토치에 맞게 데이터 순서를 바꿈\r\n",
        "        self.y_data = torch.LongTensor(y_data) # LongTensor : 64비트의 부호 있는 정수\r\n",
        "        self.len = self.y_data.shape[0]\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        return self.x_data[index], self.y_data[index]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.len"
      ],
      "outputs": [],
      "metadata": {
        "id": "-VTeaLhIool9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "train_data = TensorData(train_images, train_labels) # 외부 데이터를 텐서 형태로 불러오기\r\n",
        "train_loader = DataLoader(train_data, batch_size = 10, shuffle=True)    # 전처리를 거쳐서 배치사이즈 별로 데이터셋 만들기"
      ],
      "outputs": [],
      "metadata": {
        "id": "JhovOAPup4li"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# 생성된 데이터셋의 사이즈를 확인하자\r\n",
        "train_data[0][0].size()\r\n",
        "# torch.Size([3, 32, 32]) : 3 채널인 사이즈 32,32인 데이터"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmUwkkkKqqyV",
        "outputId": "904903f9-78ee-449d-80c7-4375fafd40f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# iter를 이용한 사이즈 확인\r\n",
        "dataiter = iter(train_loader)\r\n",
        "images, labels = dataiter.next()\r\n",
        "\r\n",
        "print(images.size())\r\n",
        "# torch.Size([10, 3, 32, 32]) : 10개 배치 사이즈로 묶인 3 채널에 사이즈 32,32인 데이터"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 32, 32])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p7sDGHuq5NV",
        "outputId": "6539a32c-9fae-4112-9ef0-7e151b17d0d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ImageFolder를 자주 쓰지 않는 이유:\n",
        "디렉토리를 함부로 바꿀 수 없는 데이터들이 있어서, 전처리가 제한 적이어서.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1xENDkEvIzK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 개인 데이터에 transform 까지 이용하기\n",
        "---"
      ],
      "metadata": {
        "id": "LhY5u-rhJAlY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "# TensorData 클래스를 이용하면 된다.\r\n",
        "\r\n",
        "class MyDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, x_data, y_data, transform=None):  # 전처리까지 하기 위해 transform 을 추가\r\n",
        "\r\n",
        "        self.x_data = x_data    # 텐서로 변환하는 것은 TOTensor 클래스로 옮김\r\n",
        "        self.y_data = y_data\r\n",
        "        self.transform = transform\r\n",
        "        self.len = len(y_data)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        sample = self.x_data[index], self.y_data[index]\r\n",
        "\r\n",
        "        if self.transform:  # 있을 때만 진행, None일 경우 생략\r\n",
        "            sample = self.transform(sample)\r\n",
        "        \r\n",
        "        return sample\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return self.len\r\n",
        "\r\n",
        "class ToTensor:     # 콜 함수를 만들어 샘플을 이 클래스를 통해 받는다\r\n",
        "    def __call__(self, sample):\r\n",
        "        inputs, labels = sample\r\n",
        "        inputs = torch.FloatTensor(inputs)\r\n",
        "        inputs = inputs.permute(2,0,1)\r\n",
        "        return inputs, torch.LongTensor(labels)\r\n",
        "\r\n",
        "# 다른 예제로 하나 더 만들어 보자\r\n",
        "class LinearTensor:\r\n",
        "    \r\n",
        "    def __init__(self, slope = 1, bias = 0):\r\n",
        "        self.slope = slope\r\n",
        "        self.bias = bias\r\n",
        "\r\n",
        "    def __call__(self, sample): # call 함수 부분에 원하는 계산들을 작성한다.\r\n",
        "        inputs, labels = sample\r\n",
        "        inputs = self.slope * inputs + self.bias\r\n",
        "\r\n",
        "        return inputs, labels"
      ],
      "outputs": [],
      "metadata": {
        "id": "2Cxy7rfII6JE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "source": [
        "# 사용하는 방법은 위와 동일하다\r\n",
        "trans = tr.Compose([ToTensor(), LinearTensor(2,5)]) # 여기서 사용한 ToTensor는 바로 위에서 정의한 ToTensor이다.\r\n",
        "dsl = MyDataset(train_images, train_labels, transform = trans)\r\n",
        "train_loader1 = DataLoader(dsl, batch_size=10, shuffle = True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "m9ThcPdXLJJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "source": [
        "# 데이터의 형태를 확인해보자\r\n",
        "first_data = dsl[0]\r\n",
        "featrues, labels = first_data\r\n",
        "print(type(featrues), type(labels))\r\n",
        "\r\n",
        "# <class 'torch.Tensor'> <class 'torch.Tensor'>\r\n",
        "# 넘파이였던 형태가 토치로 변환되었음을 확인"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "# 데이터 값 확인\r\n",
        "dataiter1 = iter(train_loader1)\r\n",
        "images1, labels1 = dataiter1.next()\r\n",
        "\r\n",
        "print(images1.size())\r\n",
        "# torch.Size([10, 3, 32, 32])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 32, 32])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "source": [
        "# 안의 값도 다 확인해보자\r\n",
        "images1[0][:5]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[341., 161., 505.,  ..., 403.,  39., 271.],\n",
              "         [ 35., 301., 407.,  ..., 269., 413.,  43.],\n",
              "         [235., 435., 379.,  ..., 293.,  59., 195.],\n",
              "         ...,\n",
              "         [347., 155., 443.,  ..., 277., 361., 115.],\n",
              "         [143.,  89.,  73.,  ..., 131., 131., 193.],\n",
              "         [351.,  27., 215.,  ..., 289.,  15.,  85.]],\n",
              "\n",
              "        [[301.,  33., 241.,  ...,  67., 223., 419.],\n",
              "         [ 19., 441., 103.,  ..., 123., 417., 255.],\n",
              "         [113., 393., 409.,  ..., 323.,  25., 225.],\n",
              "         ...,\n",
              "         [ 77., 317., 327.,  ..., 501., 347., 123.],\n",
              "         [439., 235., 269.,  ..., 515., 461., 313.],\n",
              "         [383., 363., 129.,  ...,  91., 331.,  29.]],\n",
              "\n",
              "        [[395., 121., 149.,  ..., 439., 299., 341.],\n",
              "         [ 71.,   9., 469.,  ..., 365.,  55., 403.],\n",
              "         [133., 199., 429.,  ..., 513., 239., 397.],\n",
              "         ...,\n",
              "         [139., 213.,  41.,  ...,  41.,  89.,  25.],\n",
              "         [357., 263., 445.,  ..., 393., 481., 339.],\n",
              "         [511., 447., 155.,  ..., 497., 405., 471.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\r\n",
        "### torch에서 제공하는 ToTensor 등의 전처리 기능을 사용하지 않고 왜 새로 만들어서 사용하는가?\r\n",
        "타입에러 등 제공하는 옵션과 다른 경우가 많아서..  \r\n",
        "그런데도 불구하고 제공하는 기능을 활용하고 싶다면 아래와 같이 하자\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "source": [
        "class MyDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, x_data, y_data, transform = None):\r\n",
        "        self.x_data = x_data\r\n",
        "        self.y_data = y_data\r\n",
        "        self.transform = transform\r\n",
        "        self.len = len(y_data)\r\n",
        "    \r\n",
        "    def __getitem__(self, index):\r\n",
        "        sample = self.x_data[index], self.y_data[index]\r\n",
        "\r\n",
        "        if self.transform:\r\n",
        "            sample = self.transform(sample)\r\n",
        "        \r\n",
        "        return sample\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return self.len\r\n",
        "    \r\n",
        "class MyTransform:\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        inputs, labels = sample\r\n",
        "        inputs = torch.FloatTensor(inputs)\r\n",
        "        inputs = inputs.permute(2,0,1)\r\n",
        "        labels = torch.FloatTensor(labels)\r\n",
        "\r\n",
        "        transf = tr.Compose([tr.ToPILImage(), tr.Resize(128), tr.ToTensor(), tr.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n",
        "        final_output = transf(inputs)\r\n",
        "\r\n",
        "        return final_output, labels"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "# 정의한 함수를 이용해 데이터를 처리하자\r\n",
        "ds2 = MyDataset(train_images, train_labels, transform=MyTransform())\r\n",
        "train_loader2 = DataLoader(ds2, batch_size=10, shuffle=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "source": [
        "# 데이터를 형태를 확인해보자\r\n",
        "first_data = ds2[0]\r\n",
        "features, labels = first_data\r\n",
        "print(type(features), type(labels))\r\n",
        "\r\n",
        "# <class 'torch.Tensor'> <class 'torch.Tensor'>\r\n",
        "# 텐서형태로 바뀐 것 확인"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "source": [
        "# 데이터 사이즈 확인\r\n",
        "dataiter2 = iter(train_loader2)\r\n",
        "images2, labels2 = dataiter2.next()\r\n",
        "\r\n",
        "print(images2.size())\r\n",
        "\r\n",
        "# torch.Size([10, 3, 128, 128])\r\n",
        "# 배치사이즈 10에 3채널인 128,128인 이미지"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 128, 128])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "source": [
        "#  normalize 되었는지 값을 확인해보자\r\n",
        "images2[0][:5]\r\n",
        "\r\n",
        "# 확인"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.7725,  0.7725,  0.7647,  ...,  0.4980,  0.6706,  0.6706],\n",
              "         [ 0.7725,  0.7725,  0.7647,  ...,  0.4980,  0.6706,  0.6706],\n",
              "         [ 0.5843,  0.5843,  0.5843,  ...,  0.5216,  0.7098,  0.7098],\n",
              "         ...,\n",
              "         [-0.0824, -0.0824, -0.0824,  ..., -0.4275, -0.4118, -0.4118],\n",
              "         [-0.1843, -0.1843, -0.1608,  ..., -0.5765, -0.5765, -0.5765],\n",
              "         [-0.1843, -0.1843, -0.1608,  ..., -0.5765, -0.5765, -0.5765]],\n",
              "\n",
              "        [[-0.7725, -0.7725, -0.6000,  ..., -0.8039, -0.8039, -0.8039],\n",
              "         [-0.7725, -0.7725, -0.6000,  ..., -0.8039, -0.8039, -0.8039],\n",
              "         [-0.6078, -0.6078, -0.4667,  ..., -0.6157, -0.6078, -0.6078],\n",
              "         ...,\n",
              "         [ 0.5373,  0.5373,  0.5765,  ...,  0.5216,  0.6941,  0.6941],\n",
              "         [ 0.5765,  0.5765,  0.6235,  ...,  0.4824,  0.6549,  0.6549],\n",
              "         [ 0.5765,  0.5765,  0.6235,  ...,  0.4824,  0.6549,  0.6549]],\n",
              "\n",
              "        [[-0.1059, -0.1059, -0.0510,  ..., -0.5059, -0.7176, -0.7176],\n",
              "         [-0.1059, -0.1059, -0.0510,  ..., -0.5059, -0.7176, -0.7176],\n",
              "         [-0.0902, -0.0902, -0.0275,  ..., -0.5294, -0.7098, -0.7098],\n",
              "         ...,\n",
              "         [ 0.5922,  0.5922,  0.5765,  ..., -0.3333, -0.3333, -0.3333],\n",
              "         [ 0.5451,  0.5451,  0.5451,  ..., -0.3804, -0.3647, -0.3647],\n",
              "         [ 0.5451,  0.5451,  0.5451,  ..., -0.3804, -0.3647, -0.3647]]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "metadata": {}
    }
  ]
}